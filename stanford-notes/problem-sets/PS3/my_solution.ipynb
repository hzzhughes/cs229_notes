{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set #3: Deep Learning & Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. [20 points] A Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已知：\n",
    "$$\n",
    "\\begin{align*}\n",
    "    o &= g(\\sum_{j=1}^3 w_j^{[2]} h_j + w_0^{[2]}) \\\\\n",
    "    h_j &= g(\\sum_{i=1}^2 w_{i,j}^{[1]} x_i + w_{0,j}^{[1]})\n",
    "\\end{align*}\n",
    "$$\n",
    "故有：\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial w_{1,2}^{[1]}}o &= \\frac{\\partial o}{\\partial h_2}\\frac{\\partial h_2}{\\partial w_{1,2}^{[1]}} \\\\\n",
    "    &=o(1-o)w_2^{[2]}h_2(1-h_2)x_1\n",
    "\\end{align*}\n",
    "$$\n",
    "于是：\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial w_{1,2}^{[1]}}l &= \\frac{\\partial}{\\partial w_{1,2}^{[1]}}\\frac{1}{m}\\sum_{i=1}^m(o^{(i)}-y^{(i)})^2 \\\\\n",
    "    &=\\frac{2}{m}\\sum_{i=1}^m(o^{(i)}-y^{(i)})\\frac{\\partial}{\\partial w_{1,2}^{[1]}}o^{(i)} \\\\\n",
    "    &=\\frac{2}{m}\\sum_{i=1}^m(o^{(i)}-y^{(i)})o^{(i)}(1-o^{(i)})w_2^{[2]}h_2^{(i)}(1-h_2^{(i)})x_1^{(i)}\n",
    "\\end{align*}\n",
    "$$\n",
    "进而：\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\Delta w_{1,2}^{[1]}=\\frac{2\\alpha}{m}\\sum_{i=1}^m(o^{(i)}-y^{(i)})o^{(i)}(1-o^{(i)})w_2^{[2]}h_2^{(i)}(1-h_2^{(i)})x_1^{(i)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察题目给出的散点图中class_0与class_1的点之间被一个巨大的三角形所间隔,\n",
    "而实际上$W_{3\\times2}^{[1]}x+W_0^{[1]}<0$就可以在二维空间中构建出一个三角形\n",
    "\n",
    "之后只需要将第二层神经网络构造成一个与门就可以了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若activation function $f$有$f(x)=x$，\n",
    "则实际上有：\n",
    "$$\n",
    "o=W_{1\\times3}^{[2]}(W_{3\\times2}^{[1]}x+W_0^{[1]})+W_0^{[2]}\n",
    "$$\n",
    "可化\n",
    "$$\n",
    "o=A_{1\\times2}x+b\n",
    "$$\n",
    "而显然该方程仅可在特征空间中定义一条直线，不满足题目要求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. [15 points] KL divergence and Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欲证：\n",
    "$$\n",
    "\\forall P,Q\\quad D_{KL}(P\\mid\\mid Q)\\geq 0\n",
    "$$\n",
    "及\n",
    "$$\n",
    "D_{KL}(P\\mid\\mid Q) = 0\\quad iff\\ P=Q\n",
    "$$\n",
    "可解优化问题：\n",
    "$$\n",
    "\\begin{align*}\n",
    "    &\\min_{p_i} -\\sum_i q(x_i)log(p_i)\\\\\n",
    "    &s.t.\\ \\sum_i p_i = 1\n",
    "\\end{align*}\n",
    "$$\n",
    "易见此为凸优化问题，故而可用拉格朗日法求解，先得对偶问题：\n",
    "$$\n",
    "\\begin{align*}\n",
    "    &\\min_{p_i} -\\sum_i q(x_i)log(p_i)+\\lambda(\\sum_i p_i - 1)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "进而有一阶条件：\n",
    "$$\n",
    "\\begin{cases}\n",
    "    &-q(x_i)/p_i+\\lambda=0\\\\\n",
    "    &\\sum_i p_i = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "易得上式有唯一解：$p_i=q(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欲证：\n",
    "$$\n",
    "    D_{KL}(P(X,Y)\\mid\\mid Q(X,Y)) = D_{KL}(P(X)\\mid\\mid Q(X)) + D_{KL}(P(Y\\mid X)\\mid\\mid Q(Y\\mid X))\n",
    "$$\n",
    "可有：\n",
    "$$\n",
    "\\begin{align*}\n",
    "    D_{KL}(P(X,Y)\\mid\\mid Q(X,Y)) &= \\sum_{x,y} P(x,y)log\\frac{P(x,y)}{Q(x,y)} \\\\\n",
    "    &=\\sum_{x,y} P(x,y)log\\frac{P(y\\mid x)P(x)}{Q(y\\mid x)Q(x)} \\\\\n",
    "    &=\\sum_{x,y} P(x,y)log\\frac{P(y\\mid x)}{Q(y\\mid x)}+\\sum_{x,y}P(x,y)log\\frac{P(x)}{Q(x)} \\\\\n",
    "    &=\\sum_y P(y)\\sum_x P(x\\mid y)log\\frac{P(y\\mid x)}{Q(y\\mid x)}+\\sum_x P(x)log\\frac{P(x)}{Q(x)} \\\\\n",
    "    &=D_{KL}(P(X)\\mid\\mid Q(X)) + D_{KL}(P(Y\\mid X)\\mid\\mid Q(Y\\mid X))\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欲证：\n",
    "$$\n",
    "\\arg\\min_\\theta D_{KL}(\\hat{P}\\mid\\mid P_\\theta) = \\arg\\max_\\theta \\sum_{i=1}^m log P_\\theta(x^{(i)})\n",
    "$$\n",
    "可有：\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\arg\\min_\\theta D_{KL}(\\hat{P}\\mid\\mid P_\\theta) &= \\arg\\min_\\theta \\sum_x \\hat{P}(x)log\\frac{\\hat{P}(x)}{P_\\theta(x)} \\\\\n",
    "&=\\arg\\min_\\theta \\sum_{i=1}^m \\frac{1}{m}log\\frac{\\hat{P}(x^{(i)})}{P_\\theta(x^{(i)})} \\\\\n",
    "&=\\arg\\max_\\theta \\sum_{i=1}^m log P_\\theta(x^{(i)})\n",
    "\\end{align*} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. [25 points] KL Divergence, Fisher Information, and the Natural Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\frac{\\partial}{\\partial \\theta'_i}log\\ p(y;\\theta')\\mid_{\\theta'=\\theta}=\\frac{\\partial}{\\partial \\theta'_i}p(y;\\theta')\\mid_{\\theta'=\\theta}/p(y;\\theta)$, we have:\n",
    "$$\n",
    "\\nabla_{\\theta'} log\\ p(y;\\theta')\\mid_{\\theta'=\\theta}=\\nabla_{\\theta'}p(y;\\theta')\\mid_{\\theta'=\\theta}/p(y;\\theta)\n",
    "$$\n",
    "then,\n",
    "$$\n",
    "\\begin{align*}\n",
    "    E_{y\\sim p(y;\\theta)}[\\nabla_{\\theta'} log\\ p(y;\\theta')\\mid_{\\theta'=\\theta}]\n",
    "    &=\\int_{-\\infin}^\\infin \\nabla_{\\theta'}p(y;\\theta')\\mid_{\\theta'=\\theta}dy \\\\\n",
    "    &=\\nabla_{\\theta'}\\int_{-\\infin}^\\infin p(y;\\theta')dy\\mid_{\\theta'=\\theta} \\\\\n",
    "    &=\\nabla_{\\theta'}1\\mid_{\\theta'=\\theta} \\\\\n",
    "    &=0\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    I(\\theta)\n",
    "    &=Cov_{y\\sim p(y;\\theta)}[\\nabla_{\\theta'} log\\ p(y;\\theta')\\mid_{\\theta'=\\theta}] \\\\\n",
    "    &=E_{y\\sim p(y;\\theta)}[\\nabla_{\\theta'} log\\ p(y;\\theta')\\nabla_{\\theta'} log\\ p(y;\\theta')^T\\mid_{\\theta'=\\theta}]\n",
    "\\end{align*}\n",
    "$$\n",
    "since $E_{y\\sim p(y;\\theta)}[\\nabla_{\\theta'} log\\ p(y;\\theta')\\mid_{\\theta'=\\theta}]=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove\n",
    "$$\n",
    "E_{y\\sim p(y;\\theta)}[-\\nabla_{\\theta'}^2 log\\ p(y;\\theta')\\mid_{\\theta'=\\theta}]\n",
    "=I(\\theta)\n",
    "$$\n",
    "we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "E_{y\\sim p(y;\\theta)}[-\\nabla_{\\theta'}^2 log\\ p(y;\\theta')\\mid_{\\theta'=\\theta}]\n",
    "&=E_{y\\sim p(y;\\theta)}[[-\\frac{\\frac{\\partial^2}{\\partial \\theta'_i\\partial \\theta'_j}p(y\\mid\\theta')}{p(y\\mid\\theta')}+\\frac{\\frac{\\partial}{\\partial \\theta'_i}p(y\\mid\\theta')\\frac{\\partial}{\\partial \\theta'_j}p(y\\mid\\theta')}{p^2(y\\mid\\theta')}]\\mid_{\\theta'=\\theta}]\\\\\n",
    "&=\\int_{-\\infin}^\\infin \\nabla_{\\theta'}^2p(y;\\theta')\\mid_{\\theta'=\\theta}dy\n",
    "+E_{y\\sim p(y;\\theta)}[[\\frac{\\partial}{\\partial \\theta'_i}log\\ p(y\\mid\\theta')\\frac{\\partial}{\\partial \\theta'_j}log\\ p(y\\mid\\theta')]\\mid_{\\theta'=\\theta}]\\\\\n",
    "&=0+I(\\theta)\\\\\n",
    "&=I(\\theta)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    D_{KL}(p_\\theta\\mid\\mid p_{\\theta+d})\n",
    "    &=E_{y\\sim p_\\theta}[log\\ p_\\theta-log\\ p_{\\theta+d}] \\\\\n",
    "    &\\approx E_{y\\sim p_\\theta}[d^T \\nabla_{\\theta'} log\\ p(y;\\theta')\\mid_{\\theta'=\\theta} + \\frac{1}{2}d^T\\nabla^2_{\\theta'} log\\ p(y;\\theta')\\mid_{\\theta'=\\theta}d]\\\\\n",
    "    &=E_{y\\sim p_\\theta}[\\frac{1}{2}d^T\\nabla^2_{\\theta'} log\\ p(y;\\theta')\\mid_{\\theta'=\\theta}d]\\\\\n",
    "    &=\\frac{1}{2}d^T I(\\theta)d\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    d^*\n",
    "    &=\\arg \\max_d l(\\theta+d)\\quad \n",
    "    &s.t.\\ D_{KL}(p_\\theta\\mid\\mid p_{\\theta+d})=c \\\\\n",
    "\n",
    "    &\\approx \\arg\\max_d l(\\theta)+d^T \\nabla_{\\theta'} l(\\theta')\\mid_{\\theta'=\\theta}\\quad \n",
    "    &s.t.\\ D_{KL}(p_\\theta\\mid\\mid p_{\\theta+d})=c\\\\\n",
    "\n",
    "    &= \\arg\\max_d d^T \\nabla_{\\theta'} l(\\theta')\\mid_{\\theta'=\\theta}\\quad \n",
    "    &s.t.\\ D_{KL}(p_\\theta\\mid\\mid p_{\\theta+d})=c\\\\\n",
    "\n",
    "    &\\approx \\arg\\max_d d^T \\nabla_{\\theta'} l(\\theta')\\mid_{\\theta'=\\theta}\\quad \n",
    "    &s.t.\\ d^T I(\\theta)d=2c\n",
    "\\end{align*}\n",
    "$$\n",
    "This is apparently a convex problem since the objective is affine and the only constraint is quadratic.\n",
    "\n",
    "Therefore we could use Lagrange method to approximate $d$, and here are first order conditions:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\nabla_d f_0&=\\nabla_{\\theta'}l(\\theta')\\mid_{\\theta'=\\theta}+\\lambda I(\\theta)d&=0\\\\\n",
    "    \\nabla_\\lambda f_0&=dI(\\theta)d-2c&=0\n",
    "\\end{align}\n",
    "$$\n",
    "through which we could obtain analytical form approximation:\n",
    "$$\n",
    "    d=-\\sqrt{\\frac{2c}{\\nabla_{\\theta'}^Tl(\\theta')\\mid_{\\theta'=\\theta}I^{-1}(\\theta)\\nabla_{\\theta'}l(\\theta')\\mid_{\\theta'=\\theta}}}I^{-1}(\\theta)\\nabla_{\\theta'}l(\\theta')\\mid_{\\theta'=\\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we know that \n",
    "$$\n",
    "d_{newton}=-(\\nabla_{\\theta'}^2 l(\\theta'))^{-1}\\nabla_{\\theta'} l(\\theta')\n",
    "$$\n",
    "While \n",
    "$$\n",
    "E_{y\\sim p(y;\\theta)}[-\\nabla_{\\theta'}^2 log\\ p(y;\\theta')\\mid_{\\theta'=\\theta}]\n",
    "=I(\\theta)\n",
    "$$\n",
    "we have:\n",
    "$$\n",
    "d_{natural\\ descent}=-\\sqrt{\\frac{2c}{\\nabla_{\\theta'}^Tl(\\theta')\\mid_{\\theta'=\\theta}I^{-1}(\\theta)\\nabla_{\\theta'}l(\\theta')\\mid_{\\theta'=\\theta}}}(\\nabla_{\\theta'}^2 l(\\theta'))^{-1}\\nabla_{\\theta'}l(\\theta')\\mid_{\\theta'=\\theta}\n",
    "$$\n",
    "\n",
    "> PS: I think this coincidence is actually created by the intrinsics of Taylor expansion rather than natural descent itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. [30 points] Semi-supervised EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （a）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    l_{semi-sup}(\\theta^{t+1})\n",
    "    &= \\sum_i log\\ p(x^i;\\theta^{t+1})+\\alpha l_{sup}(\\theta^{t+1})\\\\\n",
    "    &= \\sum_i log\\ \\sum_{z^i}p(x^i, z^i;\\theta^{t+1})+\\alpha l_{sup}(\\theta^{t+1})\\\\\n",
    "    &\\geq \\sum_i \\sum_{z^i} Q_i^t(z^i) log\\ \\frac{p(x^i, z^i;\\theta^{t+1})}{Q_i^t(z^i)}+\\alpha l_{sup}(\\theta^{t+1})\\\\\n",
    "    &\\geq \\sum_i \\sum_{z^i} Q_i^t(z^i) log\\ \\frac{p(x^i, z^i;\\theta^{t})}{Q_i^t(z^i)}+\\alpha l_{sup}(\\theta^{t})\\\\\n",
    "    &= \\sum_i \\sum_{z^i} Q_i^t(z^i) log\\ p(x^i;\\theta)+\\alpha l_{sup}(\\theta^{t})\\\\\n",
    "    &= \\sum_i log\\ p(x^i;\\theta)+\\alpha l_{sup}(\\theta^{t})\\\\\n",
    "    &= l_{semi-sup}(\\theta^{t})\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
