{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set #1: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. [40 points] Linear Classifiers (logistic regression and GDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) here we use the bayes' rule to obtain the posterior probability $p(y=1 \\mid x)$ :\n",
    "$$\n",
    "\\begin{align*}\n",
    "    p(y=1 \\mid x) & = \\frac{p(y=1 \\mid x)p(y=1)}{p(y=0 \\mid x)p(y=0)+p(y=1 \\mid x)p(y=1)}\\\\\n",
    "\n",
    "    & = \\frac{exp(-\\frac{1}{2}(x-\\mu_1)^T\\Sigma^{-1}(x-\\mu_1))\\phi}{exp(-\\frac{1}{2}(x-\\mu_0)^T\\Sigma^{-1}(x-\\mu_0))(1-\\phi)+exp(-\\frac{1}{2}(x-\\mu_1)^T\\Sigma^{-1}(x-\\mu_1))\\phi}\\\\\n",
    "\n",
    "    & = \\frac{1}{1+exp(-\\frac{1}{2}((x-\\mu_0)^T\\Sigma^{-1}(x-\\mu_0)-(x-\\mu_1)^T\\Sigma^{-1}(x-\\mu_1)))\\frac{1-\\phi}{\\phi}}\\\\\n",
    "\n",
    "    & = \\frac{1}{1+exp(-((\\mu_0^T\\Sigma^{-1}-\\mu_1^T\\Sigma^{-1})x+\\frac{1}{2}(\\mu_0^T\\Sigma^{-1}\\mu_0+\\mu_1^T\\Sigma^{-1}\\mu_1)+ln(\\frac{\\phi}{1-\\phi})))}\n",
    "\\end{align*}\n",
    "$$\n",
    "so it is easy to represent the $p(y=1 \\mid x)$ in form of $\\frac{1}{1+exp(-(\\theta^Tx+\\theta_0))}$,\n",
    "where $\\theta^T = \\mu_0^T\\Sigma^{-1}-\\mu_1^T\\Sigma^{-1}$,$\\theta_0 = \\frac{1}{2}(\\mu_0^T\\Sigma^{-1}\\mu_0+\\mu_1^T\\Sigma^{-1}\\mu_1)+ln(\\frac{\\phi}{1-\\phi})))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)calculate the log likelihood the first:\n",
    "$$\n",
    "\\begin{align*}\n",
    "l(\\phi,\\mu_0,\\mu_1,\\Sigma) &\\ = \\sum_{i=1}^m log(p(x^i \\mid y^i)) + \\sum_{i=1}^m log(p(y^i))\\\\\n",
    "\n",
    "&=\\sum_{i=1}^m (1-y^i)log(\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu_0)^2}{2\\sigma^2}})\n",
    "+y^ilog(\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu_1)^2}{2\\sigma^2}})\n",
    "+\\sum_{i=1}^m y^i log(\\phi) +(1-y^i)log(1-\\phi) \\\\\n",
    "\n",
    "&=\\sum_{i=1}^m (1-y^i)(-log(\\sqrt{2\\pi}\\sigma)-\\frac{(x-\\mu_0)^2}{2\\sigma^2})\n",
    "+y^i(-log(\\sqrt{2\\pi}\\sigma)-\\frac{(x-\\mu_1)^2}{2\\sigma^2})\n",
    "+\\sum_{i=1}^m y^i log(\\phi) +(1-y^i)log(1-\\phi) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Then it's easy to obtain the first order conditions:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial\\sigma} l & =  \\\\\n",
    "\\frac{\\partial}{\\partial\\mu_0} l & =  \\\\\n",
    "\\frac{\\partial}{\\partial\\mu_1} l & =  \\\\\n",
    "\\frac{\\partial}{\\partial\\phi} l & = \\sum_{i=1}^m \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Through which we are able to gain the MLE of this problem under n=1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
