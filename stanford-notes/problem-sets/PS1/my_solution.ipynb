{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set #1: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. [40 points] Linear Classifiers (logistic regression and GDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) here we use the bayes' rule to obtain the posterior probability $p(y=1 \\mid x)$ :\n",
    "$$\n",
    "\\begin{align*}\n",
    "    p(y=1 \\mid x) & = \\frac{p(y=1 \\mid x)p(y=1)}{p(y=0 \\mid x)p(y=0)+p(y=1 \\mid x)p(y=1)}\\\\\n",
    "\n",
    "    & = \\frac{exp(-\\frac{1}{2}(x-\\mu_1)^T\\Sigma^{-1}(x-\\mu_1))\\phi}{exp(-\\frac{1}{2}(x-\\mu_0)^T\\Sigma^{-1}(x-\\mu_0))(1-\\phi)+exp(-\\frac{1}{2}(x-\\mu_1)^T\\Sigma^{-1}(x-\\mu_1))\\phi}\\\\\n",
    "\n",
    "    & = \\frac{1}{1+exp(-\\frac{1}{2}((x-\\mu_0)^T\\Sigma^{-1}(x-\\mu_0)-(x-\\mu_1)^T\\Sigma^{-1}(x-\\mu_1)))\\frac{1-\\phi}{\\phi}}\\\\\n",
    "\n",
    "    & = \\frac{1}{1+exp(-((\\mu_0^T\\Sigma^{-1}-\\mu_1^T\\Sigma^{-1})x+\\frac{1}{2}(\\mu_0^T\\Sigma^{-1}\\mu_0+\\mu_1^T\\Sigma^{-1}\\mu_1)+ln(\\frac{\\phi}{1-\\phi})))}\n",
    "\\end{align*}\n",
    "$$\n",
    "so it is easy to represent the $p(y=1 \\mid x)$ in form of $\\frac{1}{1+exp(-(\\theta^Tx+\\theta_0))}$,\n",
    "where $\\theta^T = \\mu_0^T\\Sigma^{-1}-\\mu_1^T\\Sigma^{-1}$,$\\theta_0 = \\frac{1}{2}(\\mu_0^T\\Sigma^{-1}\\mu_0+\\mu_1^T\\Sigma^{-1}\\mu_1)+ln(\\frac{\\phi}{1-\\phi})))$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
